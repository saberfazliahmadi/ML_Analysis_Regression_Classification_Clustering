!pip install kneed

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import preprocessing, metrics
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report, roc_auc_score, precision_recall_curve
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import RandomizedSearchCV
from kneed import KneeLocator
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Define comprehensive pre-knowledge documentation for this project
documention = """
============================================
⚡⚡⚡ PROJECT DESCRIPTION ⚡⚡⚡
============================================

Hello there! Welcome to this Machine Learning Application.

Let's get started!

This application will help you analyze a dataset using various popular classic machine learning models.

You will be able to compare the performance of these models and determine the best one for your dataset.

This project applies various machine learning models to a supplied dataset for the purpose of comparative analysis.

The supervised classification models used include Linear Regression, Logistic Regression, Decision Tree, Random Forest, SVM, KNN and the unsupervised clustering models used include K-MEANS, Hierarchical, DBSCAN.

The main goal of the project is to identify the best model based on its performance, specifically its accuracy.

============================================
⚡⚡⚡ PROJECT USAGE ⚡⚡⚡
============================================

Machine learning models have wide-ranging real-world applications including:

- Predicting credit card fraud with logistic regression
- Forecasting retail sales using linear regression
- Classifying spam emails via naive bayes
- Identifying handwritten digits using SVMs
- Recommending products to customers with KNN
- Customer churn analysis using random forests
- Image recognition using neural networks
- This project will showcase implementations of some popular classical ML models

THIS APPLICATION HAS 4 MAIN PHASES:
PHASE 1: (FIND THE BEST POSSIBLE ML CLASSIC MODEL AND ITS BEST POSSIBLE HYPER-PARAMETERS)
PHASE 2: (ESTIMATE THE HIGHEST POSSIBLE ACCURACY OF THE BEST SELECTED MODEL ON THE BASIS OF ITS BEST HYPER-PARAMETERS)
PHASE 3: (FIND THE HIGHEST POSSIBLE ACCURACY FROM ALL AVAILABLE MODELS BASED ON THEIR BEST HYPER-PARAMETERS ON THE MAIN TEST DATA)
PHASE 4: (APPLY CLUSTERING MODELS: 4.1= K-Means++ => 4.2= Hierarchical => 4.3= DBSCAN)

Each classification model's performance is evaluated using the following metrics:

Confusion Matrix: It is a table layout that shows the true positive (TP), true negative (TN), false positive (FP), and false negative (FN) values. It provides a comprehensive view of the model's performance by comparing the predicted and actual class labels.
ROC Curve (Receiver Operating Characteristic Curve): It is a graphical plot that illustrates the diagnostic ability of a binary classifier system as the discrimination threshold (decision threshold) is varied. It shows the relationship between the true positive rate (TPR) and the false positive rate (FPR) at different threshold settings.
Accuracy: It is a commonly used metric that measures the proportion of correct predictions (true results) among the total number of cases examined. It provides an overall assessment of the model's performance.

These metrics, along with others like precision, recall, F1-score, and area under the ROC curve (AUC-ROC), can help evaluate the performance of classification models and provide insights into their strengths and weaknesses.

To use this project:
1) In first step, you should input path of your CSV dataset ( Example: /content/drive/MyDrive/preprocessed_dataset.csv )
2) In second step, enter the name of targeted feature on your Dataset ( Example: Class )
3) In third step, you should input the desired test size ( a number between range 0 an 1 - Example: 0.3 )
4) In fourth step, you should input a random state value for splitting the dataset ( any integer - Example: 101 )
5) In the fifth step, you should enter the Cross Validation, or CV pf GridSearchCV method (typically, values of CV between 3 and 10 are commonly used - Example: 10)
6) In the sixth step, you should enter 2 feature names for clustering in this format: PayloadMass , Block
7) In the seventh step, you should enter the best min_sample integer number for the DBSCAN clustering method (any integer , e.g., 3)
8) In the eighth step, you should enter the best eps float number for the DBSCAN clustering method (any float, e.g., 0.3)

============================================
⚡⚡⚡ CODE DOCUMENTATION ⚡⚡⚡
============================================

The code is structured into different sections:

1. Libraries import    : Here we import all the necessary libraries we will use in the project.
2. Functions definition: In this section, we define all the functions that will be used in the main execution of the code.
3. Main function       : This is where we execute the ML pipeline using the functions defined above.

Each function has a docstring that provides a brief explanation of its purpose, the arguments it takes (if any), and what it returns (if anything).

Note: This code is intended to be run in a Google Colab environment, hence the use of the Google Colab's drive module.
If you're running this code in a different environment, please adjust the code accordingly.

This project is subject to enhancements.
Future versions may include more models for comparison, more hyperparameter tuning options, and additional evaluation metrics.

============================================================================
⚡⚡⚡ DOCUMENTION OF best_score_ and GridSearchCV() METHODS ⚡⚡⚡
============================================================================

The `GridSearchCV()` is a method commonly used in machine learning for hyperparameter tuning.
It systematically explores a predefined range of hyperparameters for a given model, such as:
learning rates or regularization strengths, and performs cross-validation to find the, optimal combination that maximizes the model's performance.
This method helps automate the search for the best hyperparameters, enhancing the model's predictive accuracy and generalization while avoiding manual trial and error.

The best_score_ attribute in the GridSearchCV class represents the
best cross-validated score achieved by the model during the hyperparameter tuning process.
It provides a measure of how well the model performs with the selected combination of hyperparameters

- What is best_score_?
The best_score_ is a floating-point number that indicates the performance of the model with the best hyperparameters
as determined by the grid search.
It is typically a metric like accuracy, mean squared error, or another relevant measure,
depending on the problem (classification, regression, etc.).

- How is best_score_ calculated?
During the grid search, the GridSearchCV object splits the training data into multiple subsets (folds) for cross-validation.
For each combination of hyperparameters,
it trains the model on a subset (fold) of the data and evaluates its performance on another subset (not seen during training).
This process is repeated for each fold, and the average score across all folds is calculated.

- best_score_ represents the average score achieved by the model across all cross-validation folds using the best set of hyperparameters.

Example:
- Let's say you're performing grid search for a classification problem using a support vector machine (SVM) classifier.
You're tuning two hyperparameters: C (the regularization parameter) and kernel (the kernel function).

- You specify a grid of hyperparameter values, such as C = [0.1, 1, 10] and kernel = ['linear', 'rbf'].

- GridSearchCV will train and evaluate the SVM classifier with all possible combinations of:
  C and kernel (e.g., C=0.1, kernel='linear', C=1, kernel='linear', and so on).

- For each combination, it will perform k-fold cross-validation (let's say 5-fold),
which means it splits the training data into 5 subsets.

- For each combination and each fold,
it trains the SVM on 4 subsets and evaluates its performance on the remaining subset.
It calculates a performance score (e.g., accuracy) for each fold.

- After completing the cross-validation for all combinations,
it calculates the average score across all folds for each combination.

- The combination of hyperparameters that resulted in the highest average score is considered the best,
and best_score_ will be equal to this highest average score.

Interpretation:
- If best_score_ is, for example, 0.85, it means that, on average,
the model achieved an accuracy of 85% (or the specified metric) across all cross-validation folds when
using the best hyperparameters.

- You can use this best_score_ as an estimate of how well your model is expected to perform
  on unseen data when trained with the selected hyperparameters.

- It helps you choose the most suitable hyperparameters that lead to the best model performance.

- In summary, best_score_ is a crucial attribute when using grid search for hyperparameter tuning.
It guides you in selecting the hyperparameters that maximize your model's performance based on cross-validation results.

==============================================================================================================
⚡⚡⚡ DOCUMENTATION OF DIFFERENT TYPES OF METRICS TO CALCULATE DIFFERENT TYPES OF MODEL ACCURACY ⚡⚡⚡
==============================================================================================================

Let's go over each of these metrics:

## Mean Squared Error (MSE):
MSE is a popular metric for regression tasks.
It's calculated as the average of the squared differences between the predicted and actual values.
It puts more weight on large errors because they are squared in the calculation,
making it more sensitive to outliers than other metrics.

```python
def mse(actual, predicted):
    return np.mean(np.square(actual - predicted))
```

## Root Mean Squared Error (RMSE):
RMSE is just the square root of MSE.
By taking the square root, the units of the RMSE are the same as the original units of the target variable,
which can make it a more interpretable metric.

```python
def rmse(actual, predicted):
    return np.sqrt(mse(actual, predicted))
```

## Mean Absolute Error (MAE):
MAE is calculated as the average of the absolute differences between the predicted and actual values.
This metric is not sensitive to outliers as MSE or RMSE, because it does not square the errors.

```python
def mae(actual, predicted):
    return np.mean(np.abs(actual - predicted))
```

## Sum of Squared Errors (SSE):
SSE is similar to MSE, but instead of averaging the squared differences, it sums them up.
This metric is also sensitive to outliers.

```python
def sse(actual, predicted):
    return np.sum(np.square(actual - predicted))
```

## Sum of Absolute Errors (SAE):
SAE is the sum of the absolute differences between the predicted and actual values.
It's similar to MAE, but it does not average the errors. This metric is not sensitive to outliers.

```python
def sae(actual, predicted):
    return np.sum(np.abs(actual - predicted))
```
IMPORTANT NOTE: WHEN, WHY, AND HOW TO CHOOSE THESE DIFFERENT TYPES OF METRICS IN DIFFERENT SITUATIONS:
# ***⚡⚡⚡ Which approach is better to evaluate a linear regression model, and why?***⚡⚡⚡
The choice of evaluation approach for a model depends on the specific goals and characteristics of your problem.
Each evaluation metric has its strengths and weaknesses, and the best approach may vary based on the context.
Here's a breakdown of when each approach might be better:

Each of these metrics has its own use case and choosing between them depends on the specific problem and the nature of the data.
# ***⚡⚡⚡ IF YOU HAVE OUTLIERS IN YOUR DATA AND YOU WANT YOUR MODEL TO BE SENSITIVE TO THESE, THEN MSE, RMSE, AND SSE WOULD BE APPROPRIATE. # ***⚡⚡⚡
# ***⚡⚡⚡ IF YOU WANT YOUR MODEL TO BE ROBUST TO OUTLIERS, THEN MAE AND SAE WOULD BE A BETTER CHOICE.# ***⚡⚡⚡
In general, there's no one-size-fits-all answer to which evaluation approach is better.
It depends on your problem's objectives, the nature of the data, and the implications of different types of errors.
It's also a good practice to use a combination of evaluation metrics to get a comprehensive understanding of your model's performance.

Here's a guideline to help you choose:

1. **Mean Squared Error (MSE) and Root Mean Squared Error (RMSE):**
   - **When to Use:** MSE and RMSE are particularly useful when you want to penalize larger errors more heavily.
   If outliers or larger errors are critical to your problem, these metrics can help you capture their impact effectively.
   - **Why:** Squaring the errors in MSE and then taking the square root in RMSE amplifies the impact of larger errors.
   This can be advantageous if you need to minimize high-impact errors.
   - **MSE/RMSE:** Use these when you want to emphasize and penalize larger errors more heavily,
   or when squared errors are appropriate for your problem's context.

2. **Mean Absolute Error (MAE):**
   - **When to Use:** MAE is a good choice when you want to treat all errors equally, regardless of their magnitude.
   It's a robust metric in the presence of outliers and might be suitable when your data contains significant variations.
   - **Why:** Since MAE considers the absolute differences between predicted and actual values without squaring,
   it provides a balanced view of errors and is less sensitive to extreme values.
   - **MAE:** Use this when you want to treat all errors equally and are concerned about the impact of outliers.

3. **Sum of Squared Errors (SSE) and Sum of Absolute Errors (SAE):**
   - **When to Use:** SSE and SAE are more suitable when you want a holistic view of the overall error magnitude without focusing on averages.
   These metrics can be helpful for understanding the total error across the entire dataset.
   - **Why:** SSE and SAE directly sum up the squared or absolute errors, respectively, across all data points.
   This approach gives you an understanding of the total deviation between predicted and actual values.
   - **SSE/SAE:** Use these when you're interested in understanding the overall error magnitude across the entire dataset.

# ***⚡⚡⚡ IN MANY CASES, A COMBINATION OF THESE METRICS, ALONG WITH ADDITIONAL DOMAIN-SPECIFIC CONSIDERATIONS, CAN GIVE YOU THE MOST ACCURATE ASSESSMENT OF YOUR LINEAR REGRESSION MODEL'S PERFORMANCE.# ***⚡⚡⚡
# ***⚡⚡⚡ IT'S A GOOD PRACTICE TO EVALUATE YOUR MODEL USING MULTIPLE METRICS TO ENSURE A COMPREHENSIVE UNDERSTANDING OF ITS STRENGTHS AND LIMITATIONS.# ***⚡⚡⚡

Linear Regression is a good starting point for regression tasks as it's simple, fast, and often gives reasonable results.
However, it makes certain assumptions about the data, such as linearity, independence of errors,
and homoscedasticity (constant variance of errors).
If these assumptions are violated, other models may be more appropriate,
such as decision trees for non-linear relationships or ridge/lasso regression for correlated features.

Here's a summary:

| Metric | Description                | Sensitive to outliers |
| ------ | -------------------------- | --------------------- |
| MSE    | Average of squared errors  | Yes                   |
| RMSE   | Square root of MSE         | Yes                   |
| MAE    | Average of absolute errors | No                    |
| SSE    | Sum of squared errors      | Yes                   |
| SAE    | Sum of absolute errors     | No                    |

Choose Linear Regression when:
- Your data is linear or near-linear.
- The errors are independent and have constant variance.
- You need a simple, interpretable model.
- You have little to no multicollinearity among your input variables.

=====================================================================
⚡⚡⚡ DOCUMENTATION OF SVM  (Support Vector Machine) ⚡⚡⚡
=====================================================================
⚡- Some algorithms, such as SVM and Gaussian Processes, can be used with both linear and non-linear kernels, making them versatile in handling different types of problems.

⚡ What is C parameter in SVM classification?
The C parameter in SVM (Support Vector Machine) classification refers to the regularization parameter.

It controls the tradeoff between maximizing the margin between classes and minimizing misclassification errors on the training data.

In SVM classification, the C parameter, also known as the regularization parameter, controls the trade-off between achieving a wide margin and minimizing the classification errors.

It determines how much importance the SVM model assigns to correctly classifying each training example.

To understand the C parameter, let's consider a simple example. Suppose you have a dataset with two classes, represented by red and blue points, and you want to find a linear decision boundary (hyperplane) that separates the two classes.

When the C parameter is small, the SVM model prioritizes maximizing the margin, even if it means misclassifying a few training examples.
In this case, the model allows more training examples to be classified incorrectly, but it aims to achieve a wider margin between the classes.
This can lead to a more general decision boundary but could result in more training errors.
On the other hand, when the C parameter is large, the SVM model emphasizes correctly classifying as many training examples as possible.
In this case, the model tries to minimize the training errors even if it means having a narrower margin.
The decision boundary may be more complex to accommodate the correct classification of all training examples, potentially leading to overfitting if the dataset is noisy or contains outliers.

⚡ To summarize:
- Small C: Emphasizes a wider margin, potentially allowing more misclassified training examples.
- Large C: Emphasizes correct classification of training examples, potentially resulting in a narrower margin and potential overfitting.
Choosing the appropriate value of C depends on the specific problem and the characteristics of the dataset.
If you want a more flexible decision boundary and can tolerate some misclassifications, a smaller C value may be suitable.
If you prioritize accurate classification of training examples and are willing to accept a more complex decision boundary, a larger C value may be appropriate.

__________________________________________________

⚡⚡⚡USE CASES OF Gaussian Radial Basis Function (RBF) kernel⚡⚡⚡

⚡⚡⚡The Gaussian Radial Basis Function (RBF) kernel is one of the most popular kernel functions used with Support Vector Machines (SVMs) for classification⚡⚡⚡
⚡⚡⚡Here are some of the most common real-world use cases where Gaussian RBF SVMs are applied:⚡⚡⚡
⚡Image recognition - Classifying images into different categories like cats vs dogs, detecting faces, identifying handwritten digits etc. The Gaussian RBF kernel works well with high dimensional complex image data.
⚡Text classification - Categorizing documents or texts into topics, sentiment analysis, spam detection etc. Text data is high-dimensional where an RBF kernel can model non-linear relationships well.
⚡Bioinformatics - Pattern recognition in DNA sequences, protein structure prediction. Biological data is often complex and Gaussian RBF handles such complexity.
⚡Anomaly detection - Finding abnormal patterns like credit card fraud, network intrusions. The SVM model with a Gaussian kernel can detect outliers effectively.
⚡Time series analysis - Forecasting future values based on past time series data. An RBF SVM can capture nonlinear temporal relationships.
⚡Predictive maintenance - Estimating time to failure for industrial machines. Gaussian kernel is useful for modeling complex equipment failure patterns.
The key advantages of Gaussian RBF kernel that make it suitable for these use cases are:

⚡Ability to handle nonlinear decision boundaries well.
⚡Works well with high dimensional data.
⚡Flexible representation by tuning gamma and C hyperparameters.
⚡Computational efficiency for prediction compared to other kernels.
⚡So in summary, Gaussian RBF kernel excels at complex nonlinear classification tasks involving high-dimensional data like images, text, sequences, etc. Its flexibility and computational efficiency make it one of the first choices for many classification problems.

⚡⚡⚡Support Vector Machines (SVMs) with Gaussian Radial Basis Function (RBF) kernels are commonly used in real-world classification problems that involve non-linear decision boundaries⚡⚡⚡
⚡⚡⚡The RBF kernel (Gaussian Radial Basis Function) is a popular choice for SVMs due to its ability to capture complex patterns in the data⚡⚡⚡
⚡text classification =

One of the most common use cases of SVMs with Gaussian RBF kernels is in text classification. SVMs can be used to classify documents into different categories, such as spam detection, sentiment analysis, or topic classification. The RBF kernel allows the SVM to model the non-linear relationships between the words or features in the text data, enabling accurate classification.

⚡distinguishing between different objects or classes within images =

Another common application is in image classification. SVMs with RBF kernels are effective in distinguishing between different objects or classes within images. They can be trained on labeled image datasets to classify new images into predefined categories, such as recognizing handwritten digits, identifying objects in photographs, or detecting specific features in medical imaging.

⚡bioinformatics for tasks such as protein structure prediction, gene expression analysis, or disease diagnosis. =

SVMs with Gaussian RBF kernels are also used in bioinformatics for tasks such as protein structure prediction, gene expression analysis, or disease diagnosis. The RBF kernel helps capture complex relationships between biomolecular features, allowing for accurate classification or regression tasks in biological data analysis.

⚡financial analysis for tasks like stock market prediction, credit scoring, or fraud detection =

Furthermore, SVMs with RBF kernels have been applied in financial analysis for tasks like stock market prediction, credit scoring, or fraud detection. The RBF kernel can capture non-linear relationships between financial indicators and outcomes, aiding in the identification of patterns and anomalies in large datasets.

Overall, SVMs with Gaussian RBF kernels are versatile and widely used in various real-world classification problems where the decision boundaries are non-linear and complex.

They have proven effective in text classification, image classification, bioinformatics, and financial analysis, among other domains.

⚡⚡⚡The Gaussian Radial Basis Function (RBF) kernel is used for linear SVM or non linear SVM?⚡⚡⚡
⚡The Gaussian Radial Basis Function (RBF) kernel is typically used for non-linear SVMs.
The RBF kernel allows SVMs to model non-linear relationships between the input data and the target variable by mapping the data into a higher-dimensional feature space.

In a linear SVM, the decision boundary is a hyperplane that separates the data points. Linear SVMs use a linear kernel, such as the linear, polynomial, or sigmoid kernel, to perform classification in the original input space without explicitly transforming the data into a higher-dimensional space.

However, when dealing with non-linearly separable data, a linear kernel may not be sufficient. In such cases, non-linear SVMs are employed, and the RBF kernel is commonly used to capture complex patterns and non-linear relationships in the data. The RBF kernel allows the SVM to implicitly map the input data into a higher-dimensional feature space where linear separation is possible.

⚡In summary, the Gaussian RBF kernel is primarily used for non-linear SVMs, enabling them to handle non-linear classification problems by mapping the data into a higher-dimensional space.
⚡⚡⚡Is the output result of The Gaussian Radial Basis Function (RBF) kernel is to use for making a hyperplane that can classify and distinguish data in separate categories?⚡⚡⚡
Yes, that's correct! The Gaussian Radial Basis Function (RBF) kernel is used in Support Vector Machines (SVMs) to create a hyperplane that can classify and distinguish data points into separate categories.

SVMs with the RBF kernel are particularly effective in handling non-linear classification problems. The RBF kernel allows SVMs to implicitly map the data into a higher-dimensional feature space, where the data points from different classes can be separated by a hyperplane.

⚡The RBF kernel measures the similarity or distance between data points in the transformed feature space. It assigns higher similarity values to data points that are closer together and lower similarity values to data points that are farther apart. This way, the RBF kernel captures complex patterns and non-linear relationships in the data.
By using the RBF kernel, SVMs can find the optimal hyperplane that maximizes the margin between the classes in the higher-dimensional space. This hyperplane effectively separates the data points into different categories, allowing for accurate classification.

⚡In summary, the RBF kernel plays a crucial role in SVMs by transforming the data and enabling the creation of a hyperplane that can classify and distinguish data points into separate categories, even in cases where the decision boundary is non-linear.
⚡⚡⚡How the SVM model and RBF kernel draw the best possible hyperplane to separate data points into different classes?⚡⚡⚡
⚡⚡⚡STEP BY STEP PROCESS:⚡⚡⚡
The SVM model with the RBF kernel draws the best possible hyperplane to separate data points into different classes using a process called "training" or "learning." Here's a step-by-step explanation of how this is achieved:

⚡Data Preparation: First, you need a labeled dataset with input features and corresponding class labels. Each data point should be assigned a class label indicating its category or class.

⚡Kernel Selection: In the case of non-linear data, such as when classes are not separable by a straight line in the input space, the RBF kernel is commonly chosen. The RBF kernel allows for non-linear transformations that map the data into a higher-dimensional feature space.

⚡Mapping to Higher-Dimensional Space: The RBF kernel implicitly maps the input data into a higher-dimensional feature space, where it becomes possible to separate the classes using a hyperplane.

⚡Finding the Optimal Hyperplane: The SVM model aims to find the hyperplane that maximizes the margin between the classes while minimizing classification errors. The margin is the distance between the hyperplane and the closest data points from each class, known as support vectors.

⚡Optimization: The optimization process involves solving a quadratic programming problem to find the optimal Lagrange multipliers (coefficients) that determine the support vectors and the position of the hyperplane.

⚡Decision Boundary: The hyperplane determined by the SVM becomes the decision boundary. It separates the different classes in the higher-dimensional feature space.

⚡Classification: During the prediction phase, the SVM model uses the learned hyperplane to classify new, unseen data points. The model assigns the appropriate class label based on which side of the decision boundary the data point falls.

The key idea behind the SVM model is to find a hyperplane that maximizes the margin between the classes, ensuring a robust separation between them. With the RBF kernel, SVMs can handle non-linearly separable data by implicitly mapping it into a higher-dimensional space, where a hyperplane can effectively separate the classes.

By iteratively optimizing the model's parameters and maximizing the margin, SVMs with the RBF kernel draw the best possible hyperplane to separate the data points into different classes, resulting in accurate and effective classification.

___________________________________________________________

⚡⚡⚡ What is non linear SVR formula? ⚡⚡⚡
The formula for non-linear Support Vector Regression (SVR) involves the use of a non-linear kernel function, typically the Gaussian Radial Basis Function (RBF) kernel. SVR is a regression technique that uses the principles of SVM to perform non-linear regression.

⚡The general formula for non-linear SVR can be expressed as follows:

f(x) = ∑(i=1 to n) (alpha_i * K(x_i, x)) + b
In this formula:

f(x) represents the predicted output for a given input x.
alpha_i are the Lagrange multipliers (coefficients) obtained during the training phase.
K(x_i, x) is the kernel function that measures the similarity between the training sample x_i and the input x. In non-linear SVR, the RBF kernel is commonly used and the formula for the RBF kernel is: ```` K(x_i, x) = exp(-gamma * ||x_i - x||^2) ``` In the RBF kernel formula, gamma is a parameter that determines the width of the Gaussian distribution and ||x_i - x||^2 represents the Euclidean distance between x_i and x.
b is the bias term that allows for translation of the regression line.
During the training phase of non-linear SVR, the coefficients alpha_i and the bias term b are determined by solving an optimization problem that aims to minimize the prediction error while satisfying the epsilon-insensitive loss function. The optimization problem involves finding the support vectors, which are the training samples that lie closest to the regression line.

Once the model is trained and the coefficients and bias are determined, the formula allows for predicting the output f(x) for new input values x.

It's important to note that the actual implementation and specific details may vary depending on the machine learning library or framework being used. The formula provided here represents the general concept behind non-linear SVR.

____________________________________________________________

⚡⚡⚡ What are differences between non-linear SVM and non-linear SVR formula? ⚡⚡⚡

Key Differences:
⚡ The main difference between the formulas lies in the purpose of the models. Non-linear SVM is used for classification tasks, aiming to predict class labels, while non-linear SVR is used for regression tasks, aiming to predict continuous output values.
⚡ In non-linear SVM, the formula includes the class labels y_i and uses the sign() function to determine the predicted class label based on the sign of the result. This is because SVM is a binary classifier that separates data into different classes using a decision boundary.
⚡ In non-linear SVR, the formula does not involve class labels y_i or the sign() function, as it focuses on estimating the continuous output value directly. SVR aims to find a regression line that best fits the data points while considering a certain margin of allowed error (epsilon).
⚡ Overall, the primary distinction between the formulas is the inclusion of class labels and the sign() function in non-linear SVM for classification, whereas non-linear SVR focuses on estimating continuous output values without considering class labels.

_____________________________________________________________

⚡⚡⚡ what are the differences between soft margin and hard margin in SVM model of machine learning? ⚡⚡⚡

⚡ The main differences between soft margin and hard margin in Support Vector Machines (SVMs) are:

⚡ Hard Margin SVM:

Attempts to find the maximum margin hyperplane that perfectly separates the classes.
Does not allow any misclassified points.
Works well when data is linearly separable.

⚡ Soft Margin SVM:

Adds a regularization hyperparameter C to allow some misclassifications.
Allows points to lie within the margin or be on the wrong side.
Minimizes margin errors and maximizes margin simultaneously.
Useful when data is not linearly separable.

⚡ Key Differences:

Hard margin aims for perfect separation, soft margin allows some errors.
Soft margin uses regularization parameter C to control margin violations.
Hard margin may overfit data that is not linearly separable.
Soft margin generalizes better to unseen data.
Soft margin has wider application as real data is often not separable.
In summary, hard margin SVM pursues maximal separation without errors, while soft margin SVM allows some misclassifications to improve generalization ability and support non-separable data. The C parameter provides a regularization tradeoff between margin maximization and error minimization.

_________________________________________________________________

⚡In Support Vector Machines (SVM), the concepts of soft margin and hard margin refer to the handling of margin violations or misclassifications in the training data.

⚡Hard Margin SVM:

Hard margin SVM aims to find a decision boundary (hyperplane) that separates the classes with the maximum possible margin while not allowing any misclassifications.
It assumes that the training data is linearly separable, meaning there exists a hyperplane that can perfectly separate the classes.
Hard margin SVM is sensitive to outliers and noise in the data. Even a single misclassified point can make it difficult to find a feasible hyperplane.
It may not generalize well to unseen data if the assumption of linear separability does not hold.
Soft Margin SVM:

Soft margin SVM allows for some misclassifications or margin violations in the training data to achieve a balance between maximizing the margin and allowing some errors.
It is suitable for scenarios where the data might be noisy, overlapping, or not linearly separable.
Soft margin SVM introduces a slack variable for each data point, which represents the degree of violation of the margin. The objective is to minimize the sum of slacks while still maximizing the margin.
The C hyperparameter in soft margin SVM controls the trade-off between the margin width and the number of misclassifications allowed. A smaller C value allows more misclassifications and a wider margin, while a larger C value penalizes misclassifications more heavily, leading to a narrower margin.
Soft margin SVM is more robust to outliers and noise compared to hard margin SVM.
It can generalize better to unseen data by accepting a certain amount of error in the training set.
In summary, hard margin SVM seeks to find a perfect separation in linearly separable data, while soft margin SVM allows for some misclassifications and margin violations to handle noisy or overlapping data. The choice between hard margin and soft margin depends on the characteristics of the data and the level of tolerance for errors.

__________________________________________________________________

⚡Here are the differences between soft margin and hard margin in SVM:

⚡Hard Margin SVM:

Assumes the training data is linearly separable with no misclassifications.
Seeks to find a decision boundary (hyperplane) that perfectly separates the classes with the maximum possible margin.
Not tolerant to outliers or noise in the data.
May not generalize well to unseen data if the assumption of linear separability does not hold.

⚡Soft Margin SVM:

Allows for some misclassifications or margin violations in the training data.
Suitable for scenarios where the data might be noisy, overlapping, or not linearly separable.
Introduces a slack variable for each data point, representing the degree of margin violation.
Aims to find a decision boundary that maximizes the margin while still allowing a controlled number of misclassifications.
The C hyperparameter controls the trade-off between the margin width and the number of misclassifications allowed.
More robust to outliers and noise compared to hard margin SVM.
Can generalize better to unseen data by accepting a certain amount of error in the training set.
In summary, hard margin SVM assumes perfect separability and aims for a maximum-margin solution, while soft margin SVM allows for some errors and margin violations to handle more complex or noisy data. Soft margin SVM provides more flexibility and robustness, making it a preferred choice when dealing with real-world datasets.

=====================================================================
⚡⚡⚡ DOCUMENTATION OF CLUSTERING ⚡⚡⚡
=====================================================================

⚡ Since clustering is unsupervised learning and doesn’t use labeled data,
we cannot calculate performance metrics like accuracy, AUC, RMSE, etc.,
to compare different algorithms or data preprocessing techniques.
As a result, this makes it really challenging and subjective to assess the performance of clustering models.

________________________________________________________________________

⚡⚡ Can we measure the performance of clustering algorithms?
⚡ Unfortunately (or fortunately), there is no right or wrong answer in Clustering.
It would have been so simple to determine and make a statement like “X Algorithm is performing best here.” *

⚡ This is not possible and it is because of this reason clustering is a very challenging task.

⚡ Ultimately, which algorithm works better doesn’t depend on any metric that is easily
measurable but rather on the interpretation and how useful the output is for the use-case in hand.

⚡ There isn’t a sure way of measuring the performance of clustering algorithms
like you have in supervised machine learning (AUC, Accuracy, R2, etc.).

⚡ The quality of the model depends on the interpretation of output and the use case.
However, there are some workaround metrics such as Homogeneity Score, Silhouette Score, etc.

⚡⚡ Can we use clustering for feature engineering in supervised machine learning?
⚡ Yes, clustering algorithms assign labels in terms of groups in your dataset.
At the end of the day, it is a new categorical column in your dataset.
 So, clustering is often used for feature engineering in supervised learning tasks.

⚡⚡ REMEMBER, AT FIRST TO FIND THE BEST POSSIBLE NUMBER OF CLUSTERING CATEGORIES IN YOUR PROJECT (n_clusters),
YOU CAN USE DOMAIN EXPERT SUGGESTIONS, HOWEVER, YOU CAN ALSO USE THE ELBOW METHOD

________________________________________________________________________

⚡⚡⚡ What are the n_init and max_iter hyperparameters in k-means clustering method? ⚡⚡⚡
⚡⚡⚡ What are the differences between them? ⚡⚡⚡
Before diving into the specifics of the n_init and max_iter hyperparameters,
let's briefly touch on what K-means clustering is.

K-means is a popular unsupervised learning algorithm used for clustering problems,
where we aim to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean,
serving as a prototype of the cluster.
⚡ The location of the initial randomly chosen centroid in k-means++ can impact the final clusters.⚡
The key points:

In standard k-means, the initial centroids are chosen completely randomly,
which can lead to poor clustering if the random picks are bad.

k-means++ improves on this by spreading out the initial picks,
 but still starts with a random point.

So the location of this very first random centroid can still influence the end result.

To mitigate this, k-means++ has a n_init parameter that controls how many different initializations are tried.

For each n_init run, the algorithm picks a new random first centroid,
 runs k-means++, and finds the best clusters.

The final clusters are from the initialization run that gave the lowest inertia/within-cluster-sum-of-squares.

⚡ So in summary, n_init helps find a better starting point by trying multiple initializations,
 since the first random pick can impact k-means++. Using a higher n_init reduces the dependence on the first random point.⚡

⚡ n_init: ⚡
The default value of n_init in scikit-learn is 10.

Each run of the algorithm starts with random initial centroids,
and the best result (lowest inertia) across all runs is chosen as the final clustering solution.

The n_init hyperparameter determines how many times the K-means algorithm will be run with different initial centroid placements.

The n_init parameter in K-means clustering represents the number of time the K-means algorithm will be run with different centroid seeds.

⚡ In simpler terms, n_init is like taking multiple shots at finding the best starting points for the cluster centers.

Increasing the n_init value increases the chances of finding a better clustering result,
but it also increases the computational cost.

The final results will be the best output of n_init consecutive runs in terms of inertia.

⚡ This is necessary because K-means clustering is sensitive to the initial starting points (initial cluster centers),
and a bad initial start could lead to a sub-optimal clustering outcome.

⚡ max_iter: ⚡
The default value of max_iter in scikit-learn is 300.

The max_iter hyperparameter controls the maximum number of iterations the K-means algorithm will perform before considering convergence.

⚡ Each iteration involves assigning data points to the nearest centroids and updating the centroids based on the assigned points.
 If the cluster assignments and centroids do not change significantly between iterations,
 the algorithm is considered to have converged.

⚡ Setting a higher value for max_iter allows the algorithm to potentially find a better clustering solution,
especially for complex datasets or when the clusters are not well separated.
 However, a higher max_iter value also increases the computational time, particularly for large datasets.

*⚡ The max_iter parameter, on the other hand, represents the maximum number of iterations of the K-means algorithm for a single run.

*⚡ It sets a limit on how many times the algorithm is allowed to update the cluster centers and reassign points to
clusters before it says "that's enough" and stops.

*⚡In other words, max_iter is like setting a maximum number of steps that the algorithm can take to find the cluster centers.

*⚡ If the cluster centers are not changing much between steps or if the maximum number of iterations has been reached,
 the algorithm will stop.

⚡ Differences of n_init and max_iter hyperparameters in k-means clustering method:⚡
Among the key differences between n_init and max_iter are:

⚡ n_init is about how many different starting points the algorithm should try,
while max_iter is about how many steps should be taken for each single run.⚡

⚡ Adjusting n_init affects the computational cost more significantly than max_iter because
increasing n_init means running the whole algorithm multiple times,
while increasing max_iter only adds more steps within a single run.⚡

n_init helps to mitigate the problem of falling into a local minimum instead of a global minimum,
 which is a common issue in K-means due to its sensitivity to initial positions.

max_iter, meanwhile, is a control to prevent the algorithm from running endlessly in
scenarios where the clusters aren't stabilizing.

n_init controls how many times the algorithm will be run with different initial centroid placements,
choosing the best clustering result.

max_iter determines the maximum number of iterations that the algorithm will perform in each run before considering convergence.

It's important to note that these hyperparameters can impact the quality of the clustering result and the computational cost.

It's often a good practice to experiment with different values of n_init and max_iter to
find the optimal balance for a given dataset.

Additionally, scikit-learn's K-means implementation offers other optional parameters,
 such as init for centroid initialization and tol for convergence tolerance.

 These parameters can be useful for further fine-tuning the behavior of the algorithm,
  but n_init and max_iter primarily control the convergence and initialization aspects of K-means clustering.

________________________________________________________________________

⚡⚡⚡ Hierarchical Computing Distance Matrix ⚡⚡⚡
While merging two clusters we check the distance between two every pair of clusters and
 merge the pair with the least distance/most similarity.

But the question is how is that distance determined.

There are different ways of defining Inter Cluster distance/similarity.

Some of them are:

- ⚡ Min Distance : Find the minimum distance between any two points of the cluster.
- ⚡ Max Distance : Find the maximum distance between any two points of the cluster.
- ⚡ Group Average : Find the average distance between every two points of the clusters.
- ⚡ Ward’s Method : The similarity of two clusters is based on the increase in squared error when two clusters are merged.
⚡ For example, if we group a given data using different methods, we may get different results.

________________________________________________________________________

⚡⚡⚡ Hierarchical Agglomerative vs Divisive Clustering⚡⚡⚡
- Divisive clustering is more complex as compared to agglomerative clustering,
as in the case of divisive clustering we need a flat clustering method as “subroutine”
to split each cluster until we have each data having its own singleton cluster.
- Divisive clustering is more efficient if we do not generate a complete hierarchy all the way down to individual data leaves.

⚡⚡⚡ There are some clustering methods, such as "Partitional clustering (K_MEANS)" or "Hierarchical clustering", among others.

⚡⚡⚡ The partitional one is simply a division of the data set into non-overlapping clusters such that each object is in exactly one cluster.

⚡⚡⚡ However, the Hierarchical method permits clusters to have "Subclusters" , as if in a "Tree".
Each node (cluster) is the union of its children (subclusters), and the root of the tree is the cluster containing all the objects.

⚡⚡⚡ Differences between Hierarchical cases and some Partitional Ones, such us "K-Means":
Hierarchical clustering and partitional clustering methods, such as K-means, differ in several key aspects.
Here are the top most important differences between the two:

⚡Output Structure:
Hierarchical Clustering: Hierarchical clustering produces a tree-like structure known as a dendrogram.
It provides a hierarchy of clusters, allowing for both fine-grained and coarse-grained cluster assignments.

Partitional Clustering (e.g., K-means): Partitional clustering directly assigns data points to non-overlapping clusters without any hierarchical structure.

⚡Number of Clusters:
Hierarchical Clustering: Hierarchical clustering does not require specifying the number of clusters in advance.
The number of clusters can be determined by cutting the dendrogram at a certain height or using other criteria.

Partitional Clustering (e.g., K-means): Partitional clustering, including K-means, requires predefining the number of clusters before running the algorithm.

⚡Cluster Assignment:
Hierarchical Clustering: Hierarchical clustering allows for different types of cluster assignments, such as agglomerative (bottom-up) or divisive (top-down) approaches.
Partitional Clustering (e.g., K-means): Partitional clustering, like K-means, assigns each data point to one and only one cluster.

⚡Centroid Representation:
Hierarchical Clustering: Hierarchical clustering does not use explicit centroid representations. Instead, it measures the similarity or dissimilarity between clusters based on linkage criteria, such as distance or similarity measures.
Partitional Clustering (e.g., K-means): Partitional clustering, including K-means, represents each cluster by a centroid (mean or median) that serves as a representative of the cluster.

⚡Scalability:
Hierarchical Clustering: Hierarchical clustering can be computationally expensive, especially for large datasets, as it requires calculating pairwise distances or similarities between all data points.
Partitional Clustering (e.g., K-means): Partitional clustering algorithms like K-means are generally more computationally efficient and can handle larger datasets compared to hierarchical clustering.

⚡Flexibility:
Hierarchical Clustering: Hierarchical clustering can accommodate various types of distance or similarity measures and linkage criteria, allowing for more flexibility in determining cluster relationships.
Partitional Clustering (e.g., K-means): Partitional clustering methods like K-means typically rely on the Euclidean distance metric and are less flexible in adapting to different distance measures.

⚡So in summary, hierarchical builds a hierarchy of merges vs K-means partitions into a pre-set K clusters.
Hierarchical is better if you don't know the number of clusters, while K-means is faster for flat partitioning.
⚡Both Hierarchical and Partitional clustering methods (K-MEANS) have their strengths and limitations, and the choice between them depends on the specific requirements of the problem, data characteristics, and desired output structure.

________________________________________________________________________

⚡⚡⚡ DBSCAN METHOD ⚡⚡⚡
⚡⚡⚡Density-Based Spatial Clustering of Applications with Noise⚡⚡⚡
⚡⚡⚡The biggest advantage of this algorithm over K-Means and MeanShift is that it is robust to outliers, meaning it will not include outliers data points in any cluster⚡⚡⚡

=====================================================================
⚡⚡⚡ DOCUMENTATION OF WHAT ARE OUTLIERS AND NOISY DATA? ⚡⚡⚡
=====================================================================

- OUTLIERS: are data points that deviate significantly from the rest of the data, either due to measurement errors,
 data entry errors, or natural variability.
- NOISY DATA: are data points that contain random or irrelevant fluctuations that add noise to the signal.

====================================================
⚡⚡⚡ DOCUMENTATION OF CROSS-VALIDATION ⚡⚡⚡
====================================================

- Cross-validation is a crucial technique in Machine Learning for evaluating the performance of a model.

- It involves partitioning the dataset into multiple subsets, typically a training set and a validation set, multiple times.

- The model is trained and evaluated iteratively, ensuring that each data point serves as both training and validation,
thus providing a more robust estimate of its generalization performance.

- Cross-validation helps in detecting overfitting and provides a more reliable assessment of
a model's ability to perform well on unseen data, making it a fundamental tool for model selection and hyperparameter tuning.

============================================
⚡⚡⚡ DOCUMENTATION OF ROC CURVE ⚡⚡⚡
============================================

- The ROC Curve, or Receiver Operating Characteristic Curve, is a graphical representation used in Machine Learning
to assess the performance of binary classification models.

- It illustrates the trade-off between a model's true positive rate (sensitivity)
and false positive rate (1-specificity) across different decision thresholds.

- A higher area under the ROC curve (AUC-ROC) indicates better model discrimination,
with values ranging from 0.5 (random guessing) to 1 (perfect discrimination).

- The ROC curve helps visualize and compare the model's ability to distinguish between positive and negative classes,
aiding in threshold selection and model evaluation for classification tasks.

============================================
⚡⚡⚡ DOCUMENTATION OF PREPROCESSING => STANDARD SCALLING METHODS ⚡⚡⚡
============================================

# ***⚡⚡⚡WHY WE USE preprocessing.StandardScaler().fit_transform(df) ON DATA SET FEATURES?# ***⚡⚡⚡
- BECAUSE, IN SOME CASES, WE HAVE 2 FEATURES, ONE OF WHICH IS LOWER THAN THE OTHER.

- FOR EXAMPLE: (FEATURE ONE BETWEEN: 1 TO 5) (FEATURE TWO BETWEEN: 10000 TO 50000)
AND WHEN WE GIVE BOTH OF THEM TO OUR ML MODEL, THE MODEL WILL IGNORE THE LESS ONE COMPLETELY,
WHICH THE LESS ONE COULD BE VERY IMPORTANT FOR OUR BUSINESS AND CAN HAVE A VITAL EFFECT ON BUSINESS DECISIONS,
AND IT SHOULD NOT BE IGNORED BY THE ML MODEL.

- SO IN THIS SITUATION, FROM THE SKLEARN LIBRARY WE IMPORT AND USE StandardScaler().fit_transform(df) ON ALL OF OUR DATASET FEATURES.
WE DO THIS WITH THE AIM OF TRANSFORMING ALL FEATURES TO THE SAME SCALE AND

- CALCULATING THE EFFECTS OF ALL OF THEM ON OUR MODEL FITTING.

⚡- A STANDARD SCALE DATASET IS ONE WHERE ALL VARIABLES HAVE BEEN TRANSFORMED TO HAVE A MEAN CLOSE TO 0
and
⚡- A STANDARD DEVIATION CLOSE TO 1

⚡ THIS TRANSFORMATION REMOVES THE INFLUENCE OF THE ORIGINAL UNITS, MAKING DATA MORE SUITABLE FOR MEANINGFUL ANALYSIS AND MODELING.
⚡ THE MEAN TYPICALLY RANGES FROM -0.1 TO 0.1, AND THE STANDARD DEVIATION IS AROUND 0.9 TO 1.1, BUT CAN VARY DEPENDING ON THE DATASET.
__________________________________________________________________________

Definition Of Mean (Average):
The mean, also known as the average, is a measure of central tendency that represents the sum of all values in a dataset divided by the total number of values.
```
⚡ Formula of Mean (μ) = (Σxi) / n
```
Where:
- \(μ\) is the mean (average).
- \(xi\) represents each individual data point.
- \(n\) is the total number of data points in the dataset.
__________________________________________________________________________

Definition Of Standard Deviation (std):
The standard deviation is a measure of the spread or dispersion of data points in a dataset. It quantifies how much the individual data points typically deviate from the mean.
```
⚡ Formula of Standard Deviation (σ) = √((Σ(xi - μ)²) / n)
```
Where:
- \(σ\) is the standard deviation.
- \(xi\) represents each individual data point.
- \(μ\) is the mean of the dataset.
- \(n\) is the total number of data points in the dataset.
- Σ denotes the summation of the squared differences between each data point and the mean.
__________________________________________________________________________

⚡ The main difference between sklearn.preprocessing.MinMaxScaler().fit_transform()
and
sklearn.preprocessing.StandardScaler().fit_transform() ⚡

The main difference between them lies in the scaling technique used and the resulting scale of the transformed features:

- Min-Max Scaling (MinMaxScaler):

.MinMaxScaler scales the features to a specified range, typically between 0 and 1.
.It computes the minimum and maximum values of each feature in the dataset.
.The formula used to scale each feature is (x - min) / (max - min), where x is the original value.
.Min-Max scaling preserves the relative relationships between the data points but does not change the distribution or variance of the features.

- Standard Scaling (StandardScaler):

.StandardScaler scales the features to have zero mean (centered around 0) and unit variance.
.It computes the mean and standard deviation of each feature in the dataset.
.The formula used to scale each feature is (x - mean) / std, where x is the original value.
.Standard scaling transforms the features to have a mean of 0 and a standard deviation of 1.
.Standard scaling is suitable when the distribution of the features is approximately Gaussian (or bell-shaped).

⚡⚡⚡ In summary, the Min-Max scaling (MinMaxScaler) transforms the features to a specified range,
⚡⚡⚡ typically between 0 and 1, while the Standard scaling (StandardScaler) standardizes the features to have zero mean and unit variance.

⚡⚡⚡ The choice between Min-Max scaling and Standard scaling depends on the specific requirements of your data and the machine learning algorithm you plan to use.
⚡⚡⚡ Min-Max scaling is useful when you want to bound the features within a specific range,
⚡⚡⚡ while Standard scaling is suitable when you want to center the features around zero and ensure they have similar variances.

⚡⚡⚡ It's important to note that the choice of scaling technique can impact the performance of machine learning algorithms,
⚡⚡⚡ so it's recommended to experiment with different scaling methods and observe the effects on your specific dataset and problem.

Here we have several choices to apply different scalling methods on your dataset,
but here is a giude to help you find the best possible choice:
⚡⚡⚡ In the situation you described, where you have a comprehensive ML analysis that involves regression and classification models,
followed by clustering based on user-selected features,
⚡⚡⚡ I would recommend using sklearn.preprocessing.StandardScaler().fit_transform(df) as the preferred scaling technique at the start of your code.

__________________________________________________________________________

⚡⚡⚡ Here's the rationale behind this recommendation:

⚡⚡⚡ --- *** Standard Scaling (StandardScaler): ***
Python Code: sklearn.preprocessing.StandardScaler().fit_transform()

. Standard scaling transforms the features to have zero mean and unit variance.
. It is suitable when you want to center the features around zero and ensure they have similar variances.
. Standard scaling assumes that the data follows a Gaussian distribution (or is approximately normally distributed).
. Regression and classification models often assume that the features are centered around zero and have similar variances,
  which makes Standard scaling a more suitable choice in this case.

⚡⚡⚡--- *** Min-Max Scaling (MinMaxScaler): ***
Python Code: sklearn.preprocessing.MinMaxScaler().fit_transform()

. Min-Max scaling transforms the features to a specified range, typically between 0 and 1.
. It is useful when you want to bound the features within a specific range.
. While Min-Max scaling can be beneficial in some scenarios,
  it may not be the most appropriate choice for your situation,
  primarily because you're performing regression and classification analysis followed by clustering.

. Min-Max scaling does not center the features around zero or ensure similar variances,
  which might not align well with the assumptions of the regression and classification models you're using.

⚡⚡⚡ By using Standard scaling (StandardScaler), you ensure that the features are centered around zero and have similar variances.
⚡⚡⚡ This can help the regression and classification models to perform optimally, as they often assume such distributions.
⚡⚡⚡ Additionally, Standard scaling facilitates fair comparisons between different models by providing a consistent scale across features.

⚡⚡⚡ Once the scaling is applied to your dataset, you can proceed with the ML analysis,
including regression, classification, and clustering, using the scaled features.

⚡⚡⚡ Remember that the choice of scaling technique also depends on the specific characteristics of your data.
⚡⚡⚡ It's always a good practice to experiment with different scaling methods and evaluate their impact
on your specific dataset and the performance of your models.
__________________________________________________________________________
Happy analyzing!
(Author: Saber Fazliahmadi)
"""

def draw_line_separator():

    # Define the separator with dashes
    separator = '_' * 100

    # Print the separator
    print(f'\n{separator}\n')

# Remove extra spaces
def remove_space(user_input):
    """
    Remove spaces from user input.

    Args:
        user_input (str or int): Input value, can be a string or an integer.

    Returns:
        str or int: Input value with spaces removed.
    """

    # Check type of input
    # If the type of input was integer
    if isinstance(user_input, int):
      # Integer input
      user_input = str(user_input).replace(" ", "")
      user_input = int(user_input)

    # If the type of input was string
    elif isinstance(user_input, str):
      # String input
      user_input = user_input.replace(" ", "")

    return user_input

# Define a function to greet the user and get input
def greet_and_get_input():
    """
    Greet the user and get input.

    Returns:
        tuple: Tuple containing file_path, name_of_targeted_feature, test_size, random_state, and CV_number_of_GridSearchCV.
    """

    print(documention)

    draw_line_separator()

    file_path                       = remove_space(input("\nPlease enter the Path to your CSV file (e.g., /content/drive/MyDrive/preprocessed_dataset.csv): "))
    name_of_targeted_feature        = remove_space(input("\nPlease enter the name of your Target Variable or Response Variable (e.g., Class): "))
    test_size                       = remove_space(float(input("\nPlease enter the Test Size float number for train_test_split (a float number between 0 and 1, e.g., 0.3): ")))
    random_state                    = remove_space(int(input("\nPlease enter the Random State integer number for train_test_split (any integer, e.g., 101): ")))
    CV_number_of_GridSearchCV       = remove_space(int(input("\nPlease enter the integer number of Cross Validation (CV) for GridSearchCV method, this number will be used in all Classification methods (e.g., 10): ")))
    clustering_feature_names        = remove_space(input("\nPlease enter 2 feature names for clustering in this format: PayloadMass , Block "))
    min_sample_of_clustering_dbscan = remove_space(int(input("\nPlease enter the best min_sample integer number for DBSCAN clustering method (any integer , e.g., 3): ")))
    eps_of_clustering_dbscan        = remove_space(float(input("\nPlease enter the best eps float number for DBSCAN clustering method (any float , e.g., 0.3): ")))

    return file_path, name_of_targeted_feature, test_size, random_state, CV_number_of_GridSearchCV, clustering_feature_names, min_sample_of_clustering_dbscan, eps_of_clustering_dbscan

# Define a function to load data from a CSV file
def load_data(file_path):
    """
    Load data from a CSV file.

    Args:
        file_path (str): Path to the CSV file.

    Returns:
        pd.DataFrame: Loaded dataset.
    """

    try:
        data = pd.read_csv(file_path)
        print("\nData loaded successfully.\n")
        draw_line_separator()
        return data
    except Exception as e:
        print("Exception occurred while loading data: ", str(e))
        draw_line_separator()

# Define a function to apply StandardScaler to the DataFrame
def standard_scaler(df):
    """
    Apply Standard Scaler on the DataFrame.

    Args:
        df (pd.DataFrame): Input DataFrame.

    Returns:
        pd.DataFrame: DataFrame with scaled values.
    """

    # Apply a mathematic formulation on dataframe to make all data fit (standard scaling)
    scaled_df = preprocessing.StandardScaler().fit_transform(df)

    # Change the scaled_df array variable (all transformed data) to a data frame to use in the rest of this code
    scaled_df = pd.DataFrame(scaled_df, columns=df.columns)

    return scaled_df

# Define a function to check for unscaled features in the dataset
def check_unscaled_features(df):
    """
    Check columns with unscaled features.

    Args:
        df (pd.DataFrame): Input DataFrame.

    Returns:
        None
    """

    # Define threshold for mean
    MEAN_THRESH = 0.5

    # Define threshold for std
    STD_THRESH  = 0.25

    means = df.mean()
    stds  = df.std()

    # Check the rules of standard scalling on features of dataset an then find and select all unscaled features of dataset
    unscaled_cols = [col for col in df.columns if abs(means[col]) > MEAN_THRESH or abs(stds[col] - 1) > STD_THRESH]

    print("\n ⚡###COLUMNS WITH UNSCALED FEATURES:###⚡ \n", unscaled_cols)
    print('\n Head of first 10 rows of dataset: \n', df.head(10))
    print(f'\n Mean of the first feature of dataset ({df.columns[0]}): {df.iloc[:, 0].mean()}')
    print(f'\n Mean of the second feature of dataset ({df.columns[1]}): {df.iloc[:, 1].mean()}')
    print(f'\n Mean of the third feature of dataset ({df.columns[2]}): {df.iloc[:, 2].mean()}')
    print(f'\n Standard Deviation of the first feature of dataset ({df.columns[0]}): {df.iloc[:, 0].std()}')
    print(f'\n Standard Deviation of the second feature of dataset ({df.columns[1]}): {df.iloc[:, 1].std()}')
    print(f'\n Standard Deviation of the third feature of dataset ({df.columns[2]}): {df.iloc[:, 2].std()}')

    draw_line_separator()

# Define remove non numeric columns function
def remove_non_numeric_columns(data):
    # Get a list of all the columns names and data types
    dtypes = data.dtypes

    # Create empty SET to store non-numeric column names
    # Sets in Python only contain unique elements.
    # So this will automatically handle removing any duplicates as we build up the list of non-numeric column names.
    non_numeric_cols = set()

    # Loop through each column name and data type
    for col, dtype in dtypes.iteritems():

        """
          Specifically, np.number encapsulates the following NumPy numeric types:
          np.float64 - Standard double-precision floating point
          np.float32 - Standard single-precision floating point
          np.int64 - Standard 64-bit integer
          np.int32 - Standard 32-bit integer
          np.int16 - Standard 16-bit integer
          np.int8 - Standard 8-bit integer
          np.uint64 - Standard 64-bit unsigned integer
          np.uint32 - Standard 32-bit unsigned integer
          np.uint16 - Standard 16-bit unsigned integer
          np.uint8 - Standard 8-bit unsigned integer
        """
        # Compare data type against numpy numeric types
        if dtype not in [np.number]:

            # If non-numeric, append column name to list
            non_numeric_cols.add(col)

    # Convert set to list
    non_numeric_cols = list(non_numeric_cols)

    # Print list of non-numeric columns
    print(f'List of non-numeric columns: {non_numeric_cols}')

    # Drop all non-numeric columns from the DataFrame
    data = data.drop(columns=non_numeric_cols)

    # Print remaining numeric-only columns
    print(f'Remaining numeric-only columns: {data.columns}')

    return data

# Define a function to perform GridSearchCV on a model
def grid_search(model, parameters, x_train, y_train, CV_number_of_GridSearchCV):
    """
    Perform GridSearchCV on a model.

    Args:
        model: The machine learning model.
        parameters (dict): Hyperparameters to search.
        x_train (pd.DataFrame): Training features.
        y_train (pd.Series): Training labels.
        CV_number_of_GridSearchCV (int): Number of cross-validation folds which was created by user input

    Returns:
        GridSearchCV: The trained GridSearchCV object.
    """

    cv = GridSearchCV(model , parameters, cv = CV_number_of_GridSearchCV)
    cv.fit(x_train, y_train)

    # return the fited cv model based on x_train and y_train
    return cv

# Define a function to plot the confusion matrix
def plot_confusion_matrix(y, y_predict, model_name):
    """
    Plot the Confusion Matrix.

    Args:
        y (pd.Series): True labels.
        y_predict (pd.Series): Predicted labels.
        model_name (str): Name of the model.

    Returns:
        None
    """

    draw_line_separator()

    print(f'⚡This classification_report calculated by x_test and y_test for {model_name} model\n')
    print(classification_report(y, y_predict))

    draw_line_separator()

    print(f'⚡This confusion_matrix calculated by x_test and y_test for {model_name} model\n')
    cm = confusion_matrix(y, y_predict)
    ax = plt.subplot()
    sns.heatmap(cm, annot=True, ax=ax)
    ax.set_xlabel('Predicted Labels')
    ax.set_ylabel('True Labels')
    ax.set_title('Confusion Matrix')
    plt.show()

# Define a function to plot the ROC curve
def plot_roc_curve(y, y_predict, model_name):
    """
    Plot the ROC Curve.

    Args:
        y (pd.Series): True labels.
        y_predict (pd.Series): Predicted probabilities.
        model_name (str): Name of the model.

    Returns:
        None
    """

    draw_line_separator()

    print(f'⚡This roc_curve calculated by x_test and y_test data for {model_name}\n')
    fpr, tpr, _ = roc_curve(y, y_predict)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'{model_name} ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve for {model_name}')
    plt.legend(loc='lower right')
    plt.show()

# Define a function to plot the Precision-Recall curve
def plot_precision_recall_curve(y, y_predict, model_name):
    """
    Plot the Precision-Recall Curve.

    Args:
        y (pd.Series): True labels.
        y_predict (pd.Series): Predicted probabilities.
        model_name (str): Name of the model.

    Returns:
        None
    """

    draw_line_separator()

    print(f'⚡This precision_recall_curve calculated by x_test and y_test data for {model_name}\n')
    precision, recall, _ = precision_recall_curve(y, y_predict)

    plt.figure(figsize=(8, 6))
    plt.step(recall, precision, color='b', alpha=0.2, where='post')
    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title(f'Precision-Recall Curve for {model_name}')
    plt.show()

# PHASE 4.1: (CLUSTERING SECTION k-means++)
# Define the function of clustering k_means_plus_plus
def clustering_k_means_plus_plus_section(clustering_feature_names, data):
    """
    Perform the clustering section.

    Args:
        clustering_feature_names (str): Feature names for clustering.
        data (DataFrame): Input data.

    Returns:
        None
    """
    from sklearn.cluster import KMeans

    # PHASE 4.1: (CLUSTERING SECTION)
    print('⚡⚡⚡PHASE 4.1: CLUSTERING K-MEANS++ SECTION⚡⚡⚡')

    # Define the feature names for clustering
    clustering_feature_names = clustering_feature_names.split(",")

    # Define the feature name for the x-axis of the scatter plot
    plot_scatter_feature_name_as_x = clustering_feature_names[0]

    # Define the feature name for the y-axis of the scatter plot
    plot_scatter_feature_name_as_y = clustering_feature_names[1]

    # Define the initialization method for k-means
    init_name = 'k-means++'

    # Define the number of iterations for k-means
    n_init_number = 12

    """
    Define x variable
    and select clustering_feature_names from all features of the main dataset
    and then make the x variable as an array type variable from our main dataset
    """
    x = data[clustering_feature_names].values

    # Use nan_to_num on x
    x = np.nan_to_num(x)

    # Make all values in x as standard values via fit_transform(x) from StandardScaler() of sklearn library
    standard_scaled_dataset = StandardScaler().fit_transform(x)

    # Print the new Standard Scaled data
    print(f'Here are the first 30 values of the Standard Scaled Dataset for Clustering {clustering_feature_names}:\n{standard_scaled_dataset[:30]}\n')

    """
    The Elbow method aims to find the number of clusters at which the decrease in within-cluster sum of squares (WCSS) starts
    to level off. The WCSS is a measure of the compactness of the data points within each cluster.
    The Elbow method plots the number of clusters against the corresponding WCSS values and looks for an "elbow" point,
    which indicates the optimal number of clusters.

    The optimal clustering number based on the Elbow Method
    indicates the number of clusters where the decrease in
    within-cluster sum of squares (WCSS) levels off.
    It is determined by finding the knee point on the WCSS curve.
    The knee value can be accessed using kneedle.knee.

    The kneedle.knee in your code refers to the knee value calculated by the KneeLocator class from the kneed library.
    The knee value represents the optimal number of clusters based on the Elbow method,
    which is a technique used to determine the appropriate number of clusters in a dataset.
    """

    # Initialize WCSS list
    wcss = []

    # Calculate optimal clustering number via Elbow method
    for i in range(1, 11):
        k_means_model_temp = KMeans(init=init_name, n_clusters=i, n_init=n_init_number, random_state=42)
        k_means_model_temp.fit(data[clustering_feature_names].values)
        wcss.append(k_means_model_temp.inertia_)

    # Calculate the best clustering number via Elbow method
    kneedle = KneeLocator(range(1, 11), wcss, curve='convex', direction='decreasing')
    clustering_number = kneedle.knee

    # Print the optimal clustering number
    print(f'The Optimal clustering number based on the Elbow Method is: {clustering_number}')

    # Plot elbow curve
    plt.plot(range(1, 11), wcss)
    plt.xlabel('Number of Clusters')
    plt.ylabel('WCSS')
    plt.title(f'Elbow Method - Optimal K = {clustering_number}')
    plt.show()

    # Fit and predict with final clustering number
    k_means_model_final = KMeans(init=init_name, n_clusters=clustering_number, n_init=n_init_number, random_state=42)

    # Get the cluster labels
    cluster_labels_final = k_means_model_final.fit_predict(standard_scaled_dataset)

    # Get the cluster centers
    k_means_cluster_centers_final = k_means_model_final.cluster_centers_

    # Print the cluster centers
    print(f'\nOur k-means cluster centers are {len(k_means_cluster_centers_final)} categories:\n{k_means_cluster_centers_final}')

    # Add the cluster labels to the data
    data['cluster_labels'] = cluster_labels_final

    # Print a message indicating the features used for clustering and the scatter plot
    print(f'We used these feature names from all features of our dataset for our clustering: {clustering_feature_names} and \nThis feature name as X point of our plot scatter: {plot_scatter_feature_name_as_x} and \nThis feature name as Y point of our plot scatter: {plot_scatter_feature_name_as_y}')

    # Create the scatter plot
    plt.scatter(standard_scaled_dataset[:,0], standard_scaled_dataset[:,1], c=cluster_labels_final.astype(np.float), alpha=0.5 )
    plt.scatter(k_means_cluster_centers_final[:, 0], k_means_cluster_centers_final[:, 1], s=100, c='red', label='Centroids')

    # Add axis labels and a title to the plot
    plt.xlabel(plot_scatter_feature_name_as_x)
    plt.ylabel(plot_scatter_feature_name_as_y)
    plt.title(f'Figure of Final {init_name} Clustering Result of {", ".join(clustering_feature_names)} \n Thanks to KneeLocator library we Calculated the best clustering number via Elbow method is {clustering_number}')
    plt.legend()
    plt.show()

# PHASE 4.2: (CLUSTERING SECTION Hierarchical)
# Define the function of clustering Hierarchical
def clustering_hierarchical_section(data):
    """
    Perform the hierarchical clustering section.

    Args:
        data (DataFrame): Input data.

    Returns:
        None
    """
    from scipy.cluster.hierarchy import dendrogram, linkage
    from scipy.spatial.distance import pdist, squareform
    from sklearn.cluster import AgglomerativeClustering

    # PHASE 4.2: (CLUSTERING Hierarchical SECTION)
    print('⚡⚡⚡PHASE 4.2: CLUSTERING Hierarchical SECTION⚡⚡⚡')

    feature_mtx = StandardScaler().fit_transform(data.values)

    # Assuming your data is stored in a NumPy array called 'feature_mtx'
    # Calculate pairwise distances using Euclidean distance
    """
    The feature_mtx variable should be a NumPy array, and it doesn't necessarily have to be a square matrix.
    The pdist function from scipy.spatial.distance is designed to handle pairwise distance calculations efficiently for various types of input arrays.

    The feature_mtx array represents your dataset, where each row typically represents a data point, and each column represents a feature.

    The pdist function calculates the pairwise distances between all pairs of points in the feature_mtx array using the specified distance metric,
    which in this case is the Euclidean distance (metric='euclidean').

    The resulting distances variable will be a condensed distance matrix, which is a one-dimensional array containing the lower triangular part of the pairwise distance matrix.
    It stores the distances between each pair of data points in a condensed form to save memory and computation time.
    """
    distances = pdist(feature_mtx, metric='euclidean')

    print(f'The Shape of our distance Matrix Before applying squareform() method is: {distances.shape}')

    """
    - distance_matrix: This is the input distance matrix that represents the dissimilarities between pairs of samples.
    - It is a square matrix where each element represents the distance or dissimilarity between two samples.
    - The distance matrix is precomputed based on the specific distance "metric" you choose for your data.
    """
    # Convert the pairwise distances to a square distance matrix
    distance_matrix = squareform(distances)

    """
    In this example, distance_matrix will be a square matrix where each element distance_matrix[i, j] represents the distance between the i-th and j-th data points.

    Both the condensed distance matrix (distances) and the squareform distance matrix (distance_matrix) contain the same pairwise distances.
    The only difference is the representation and indexing of the distances.

    ⚡⚡⚡So, to reiterate, when you convert a condensed distance matrix to a squareform distance matrix using squareform, your data is not lost or removed.
    ⚡⚡⚡It remains intact, and you simply obtain a different representation of the pairwise distances.

    when you use squareform(distances) to convert the condensed distance matrix into a squareform distance matrix, your data is not lost or removed.
    The conversion from a condensed distance matrix to a squareform distance matrix does not alter or remove any data. It simply rearranges the condensed distances into a square matrix form.

    To clarify, the squareform function in scipy.spatial.distance takes a condensed distance matrix as input and returns a square matrix where
    the pairwise distances are arranged in a symmetric manner.
    This conversion is useful when you need to work with a matrix representation where each
    element corresponds to the distance between two data points.

    ⚡⚡⚡The process of converting a condensed distance matrix to a squareform distance matrix does not remove or discard any data.
    ⚡⚡⚡It rearranges the distances in a way that facilitates easier indexing and interpretation of the pairwise distances.
    """

    print(f'The Shape of our distance Matrix After applying squareform() method is: {distance_matrix.shape}')

    print('Our distance_matrix After Applying squareform:\n', distance_matrix)

    """
    linkage: This is a function from the scipy.cluster.hierarchy module that calculates the linkage matrix.
    The linkage matrix contains the hierarchical clustering information, including the merging order and distances between clusters.
    It takes the distance matrix as input, along with the chosen linkage method.

    method='ward': The method parameter specifies the linkage method to be used.
    "Ward" linkage is a popular method that minimizes the variance within each cluster and aims to create compact and balanced clusters.
    Other commonly used linkage methods include "single," "complete," and "average," each with its own characteristics and use cases.

    By calling linkage(distance_matrix, method='ward'),
    the code computes the linkage matrix using the "ward" linkage method based on the provided distance matrix.
    The resulting linkage_matrix variable contains the hierarchical clustering information, which can be used to plot the dendrogram.

    The choice of linkage method in hierarchical clustering depends on the specific characteristics of your data and the goals of your analysis.
    Here are a few common linkage methods and their considerations:

    1. Ward linkage (`method='ward'`):
      - The Ward linkage method aims to minimize the variance within each cluster when merging clusters.
      - It tends to produce compact and well-separated clusters.
      - It is suitable when you want to identify clusters with similar variances or sizes.
      - This method is often a good choice when you seek balanced and evenly sized clusters.

    2. Complete linkage (`method='complete'`):
      - The complete linkage method considers the maximum distance between pairs of data points from different clusters when merging clusters.
      - It tends to create clusters with similar diameters.
      - It is suitable when you want to identify clusters with distinct boundaries or when dealing with outliers.
      - This method can handle uneven-sized clusters but may be sensitive to noise or outliers.

    3. Average linkage (`method='average'`):
      - The average linkage method calculates the average distance between all pairs of data points from different clusters when merging clusters.
      - It tends to create clusters with intermediate sizes and can work well in many scenarios.
      - It is suitable when you want to balance between compactness and connectivity of clusters.

    4. Single linkage (`method='single'`):
      - The single linkage method considers the minimum distance between pairs of data points from different clusters when merging clusters.
      - It tends to form long, chain-like clusters.
      - It is suitable when you want to identify clusters with linear structures or when dealing with non-globular data.
      - This method can be sensitive to noise or outliers.

    ⚡⚡⚡The choice of linkage method should be guided by the specific characteristics of your dataset and the objectives of your analysis.
    ⚡⚡⚡It is often a good practice to try multiple linkage methods and compare the resulting clusters based on evaluation metrics or domain knowledge.

    ⚡⚡⚡Consider experimenting with different linkage methods in your hierarchical clustering analysis to determine which one best suits your dataset and the goals of your study.

    ______________________________________________

    ⚡⚡⚡Based on the situation you described, where you have performed a comprehensive machine learning analysis and want to hierarchically cluster the dataset,
    I would recommend using 'average' linkage for the linkage method.⚡⚡⚡

    ⚡⚡⚡Average linkage looks at the average distance between clusters when determining how to link them in the hierarchy.
    Some advantages of average linkage:

    - Minimizes the impact of noise and outliers compared to other methods like single or complete linkage. This makes it more robust.

    - Avoids chaining behavior that can happen with single linkage, where clusters may be forced together due to single data points.

    - Computationally fast compared to complete linkage.

    - Works well on many different types of compact and elongated clusters in the data.

    - Generally produces balanced clustering solutions.

    Since you already have a distance matrix calculated between your data points, average linkage is a good choice to generate the dendrogram and cluster hierarchy from that.

    Other valid choices could be:

    - Complete linkage: Focuses on maximum distances when linking and avoids chaining effects. Can be slow.

    - Single linkage: Focuses on minimum distances. Can cause chaining effects.

    - Ward's linkage: Uses an ANOVA approach to minimize variance when linking clusters. Performs well but can be slow.

    ⚡⚡⚡ So in summary, I would recommend starting with average linkage as a good balance of performance and clustering quality for your hierarchical clustering analysis.
    But you may want to experiment with some other methods as well.
    """
    # Calculate the linkage matrix based on the distance matrix
    linkage_matrix = linkage(distance_matrix, method='average')

    # Plot the dendrogram horizontally
    plt.figure(figsize=(10, 6))
    plt.title('Hierarchical Clustering Dendrogram')
    plt.xlabel('Distance')
    plt.ylabel('Sample Index')
    dendrogram(linkage_matrix, orientation='left', leaf_rotation=0, leaf_font_size=8, distance_sort='descending', show_leaf_counts=True)
    plt.gca().invert_xaxis()
    plt.show()

    # Perform hierarchical clustering using the AgglomerativeClustering algorithm
    clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage='average')

    """
    The hierarchical clustering, in general, does not use explicit centroid representations.
    However, in the case of the `AgglomerativeClustering` algorithm in scikit-learn,
    the `n_clusters` parameter is used to specify the desired number of clusters to be formed.

    The `n_clusters` parameter in `AgglomerativeClustering` specifies the number of clusters the algorithm should try to form.
    It does not imply the use of explicit centroids like in k-means clustering,
    but rather indicates the desired number of clusters as an output of the algorithm.

    In hierarchical clustering, the algorithm starts with each data point as an individual cluster and
    then progressively merges clusters until the desired number of clusters is reached.
    The merging process is based on a linkage criterion, specified by the `linkage` parameter in `AgglomerativeClustering` (e.g., "ward", "complete", "average", etc.),
    which determines how the distance between clusters is calculated.

    When `n_clusters` is set to `None` (the default value), the algorithm will continue merging clusters until only 1 cluster remains.
    In this case, the resulting dendrogram can be inspected to determine the appropriate number of clusters based on the desired criteria,
    such as the heights at which branches merge or the dissimilarity between clusters.

    ⚡⚡⚡ However, if you have prior knowledge or a specific requirement for the number of clusters, you can set `n_clusters` to the desired value.
    ⚡⚡⚡ The algorithm will then stop merging clusters once the specified number of clusters is reached.

    ⚡⚡⚡ To summarize, while hierarchical clustering does not use explicit centroid representations,
    ⚡⚡⚡ the `n_clusters` parameter in `AgglomerativeClustering` allows you to indicate the desired number of clusters to be formed during the merging process.
    """

    clustering.fit(distance_matrix)

    data['hierarchical_cluster'] = clustering.labels_

    print('Here you can see data.head(10):' , data.head(10))

# PHASE 4.3: (CLUSTERING SECTION DBSCAN)
# Define the function for clustering DBSCAN section
def clustering_DBSCAN_section( clustering_feature_names, data, CV_number_of_GridSearchCV, min_sample_of_clustering_dbscan, eps_of_clustering_dbscan ):
    """
    Perform the clustering section.

    Args:
        clustering_feature_names (str): Feature names for clustering.
        data (DataFrame): Input data.
        CV_number_of_GridSearchCV:

    Returns:
        None
    """
    from sklearn.cluster import DBSCAN

    # PHASE 4.3: (CLUSTERING SECTION)
    print('⚡⚡⚡PHASE 4.3: CLUSTERING DBSCAN SECTION⚡⚡⚡')

    # Define the feature names for clustering
    clustering_feature_names = clustering_feature_names.split(",")

    # Define the feature name for the x-axis of the scatter plot
    plot_scatter_feature_name_as_x = clustering_feature_names[0]

    # Define the feature name for the y-axis of the scatter plot
    plot_scatter_feature_name_as_y = clustering_feature_names[1]

    """
    Define x variable
    and select clustering_feature_names from all features of the main dataset
    and then make the x variable as an array type variable from our main dataset
    """
    x = data[clustering_feature_names].values

    """
    Using `np.nan_to_num` and StandardScaler before DBSCAN can be useful preprocessing steps in some cases:
    - `np.nan_to_num` replaces NaN values with zeros. This handles any missing data in the dataset.
    DBSCAN requires finite numeric values in the feature space.
    - StandardScaler standardizes the features by removing the mean and scaling to unit variance.
    This can be helpful for DBSCAN for a couple reasons:
    - It prevents features on larger scales from dominating the distance computations.
    After standardization, all features are on a comparable scale.
    - It makes the eps parameter more consistent across different datasets.
    Easier to set eps values that lead to meaningful clustering.
    However, standard scaling is not strictly required for DBSCAN - it can work on the raw data as well.
    Some considerations:
    - If features already on similar scales, scaling may not help much.
    - Can try DBSCAN with and without scaling to see impact.
    - Other preprocessing like dimensionality reduction can also help.
    So in summary, `nan_to_num` is recommended to handle NaNs before cluster modeling.
    And standard scaling can potentially improve DBSCAN clustering, but is not mandatory.
    Try with and without scaling on a dataset to understand the impact.
    The optimal preprocessing depends on the particular dataset characteristics.
    """
    # Use nan_to_num on x
    x = np.nan_to_num(x)

    # Make all values in x as standard values via fit_transform(x) from StandardScaler() of sklearn library
    standard_scaled_dataset = StandardScaler().fit_transform(x)

    """
    We instantiate the DBSCAN model with an epsilon (eps) and min_sample values that we gave from user
    Then we fit the DBSCAN model to our sample data.

    Some key points when using DBSCAN with high-dimensional data:
    - Density measurements can become less meaningful in very high dimensions. Adjust eps carefully.
    - Certain distance metrics like Haversine are limited to 2-3 dimensions.
    - Expect higher runtimes as dimensionality increases due to distance calculations.
    - May need more points to find meaningful clusters.
    - Can combine with dimensionality reduction techniques like PCA.
    So in summary, yes DBSCAN can handle multi-dimensional data as long as an appropriate distance metric is used.
    The efficacy in very high dims may decrease, so tuning eps and preprocessing with dimensionality reduction can help.
    """
    # Instantiate and fit the DBSCAN model
    dbscan = DBSCAN(eps=eps_of_clustering_dbscan, min_samples=min_sample_of_clustering_dbscan)
    dbscan.fit(standard_scaled_dataset)

    """
    We extract the cluster labels assigned by DBSCAN to each data point.
    We also create a boolean mask to identify the core samples
    (points with enough neighbors) using the core_sample_indices_ attribute of the DBSCAN model.
    """
    # Extract cluster labels and core sample indices
    labels = dbscan.labels_
    core_samples_mask = np.zeros_like(labels, dtype=bool)
    core_samples_mask[dbscan.core_sample_indices_] = True

    print(set(core_samples_mask))
    print(f'core_samples_mask is:\n{core_samples_mask}\n')

    """
    We determine the number of clusters by counting the unique labels assigned by DBSCAN.
    If there are noise points (labeled as -1), we subtract 1 from the count.
    We also count the number of noise points specifically.
    """
    # Number of clusters in labels, ignoring noise if present
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise = list(labels).count(-1)

    print(f'n_clusters:\n{n_clusters}\n')
    print(f'n_noise is:\n{n_noise}\n')

    """
    We create a set of unique labels and generate a list of colors
    using a spectral colormap from matplotlib.pyplot.
    Each cluster will be assigned a different color.
    """
    # Plot the DBSCAN clusters
    unique_labels = set(labels)
    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]

    for k, col in zip(unique_labels, colors):
        if k == -1:
            # Black used for noise points
            col = [0, 0, 0, 1]

        class_member_mask = (labels == k)

        xy = standard_scaled_dataset[class_member_mask & core_samples_mask]
        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)

        xy = standard_scaled_dataset[class_member_mask & ~core_samples_mask]
        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=3)

    """
    We iterate over the unique labels and corresponding colors.
    For each label, we check if it represents noise (-1). If so, we set the color to black.
    Then, we create a boolean mask to identify the data points belonging to the current label.
    We plot the core samples with larger markers and non-core samples with smaller markers, using the specified color.
    """
    plt.title(f'DBSCAN Clustering \n Core samples are represented with larger markers, while non-core samples are represented with smaller markers \n Noise points are shown in black \n the min_sample_of_clustering_dbscan is {min_sample_of_clustering_dbscan} \n eps_of_clustering_dbscan is {eps_of_clustering_dbscan} ')
    plt.xlabel(plot_scatter_feature_name_as_x)
    plt.ylabel(plot_scatter_feature_name_as_y)
    plt.show()

# Create the main function to manage the whole process of this ML application
def main():
    """
    Main function to to manage the whole process of this ML application.

    Returns:
        None
    """

    # Greet the user and get all essnetial inputs
    file_path, name_of_targeted_feature, test_size, random_state , CV_number_of_GridSearchCV , clustering_feature_names , min_sample_of_clustering_dbscan, eps_of_clustering_dbscan = greet_and_get_input()

    # Load data
    data = load_data(file_path)

    # Get names of all features
    names_of_all_features = data.columns.tolist()

    # Remove the specific feature name from the list of all features names
    name_of_all_features_except_the_targeted_feature = [feature for feature in names_of_all_features if feature != name_of_targeted_feature]

    print("Path of main dataset rows              : ", file_path)
    print("Shape of main dataset                  : ", data.shape)
    print("Names of all features on your dataset  : ", names_of_all_features)
    print("Names of your target feature           : ", name_of_targeted_feature)
    print("Names of your target feature for Clustering: ", clustering_feature_names)

    draw_line_separator()

    # Get number of rows
    num_rows    = data.shape[0]
    # Get number of columns
    num_columns = data.shape[1]

    print("Number of main dataset rows, before apply dropna() method on dataset: "   , num_rows)
    print("Number of main dataset columns, before apply dropna() method on dataset: ", num_columns)
    print("Number of all NaN values on my dataset, before dropping the NaN rows via dropna() method: ", data.isna().sum().sum())
    print("Number of all Null values on my dataset, before dropping the Null rows via dropna() method: ", data.isnull().sum().sum())

    # START to Preprocess Data

    # Lets drop all rows contain at least 1 null values in one of their columns
    """ dropna() method in pandas DataFrame drops all the rows that contain null values (NaN or None) by default.
    It removes any row that has at least one null value in any of its columns.
    """
    data = data.dropna()
    data = data.reset_index(drop=True)

    draw_line_separator()

    # Get number of rows
    num_rows    = data.shape[0]
    # Get number of columns
    num_columns = data.shape[1]

    print("Number of main dataset rows, after apply dropna() method on dataset: "   , num_rows)
    print("Number of main dataset columns, after apply dropna() method on dataset: ", num_columns)
    print("Number of all NaN values on my dataset, after dropping the NaN rows via dropna() method: ", data.isna().sum().sum())
    print("Number of all Null values on my dataset, after dropping the Null rows via dropna() method: ", data.isnull().sum().sum())

    draw_line_separator()

    # Get the cleaned data by removing non numeric columns
    #data = remove_non_numeric_columns(data)

    draw_line_separator()

    # Split the dataset into features (X) and the target variable (y)
    X = data.drop(name_of_targeted_feature, axis='columns')
    Y = data[name_of_targeted_feature]

    # Check for Standard Scaling Before applying standard scaling method on the dataset
    print('⚡Before call standard_scaler(X) method we check to be sure our features have a standard scalling or not, here you can see the result:')
    check_unscaled_features(X)

    # Apply standard scaling method on the dataset
    X = standard_scaler(X)

    draw_line_separator()

    # Check for Standard Scaling After applying standard scaling method on the dataset
    print('⚡After call standard_scaler(X) method we check to be sure our features have a standard scalling or not, here you can see the result:')
    check_unscaled_features(X)

    # Check for all Null data before splitting
    print("null values of X:", X.isnull().values.any())
    print("null values of Y:", Y.isnull().values.any())

    # Split the dataset into training and testing datasets
    # Split X and Y separately
    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=random_state)

    draw_line_separator()

    # Verify and print shapes of x_train, x_test, y_train, y_test
    print("x_train shape:", x_train.shape)
    print("x_test shape:" , x_test.shape)
    print("y_train shape:", y_train.shape)
    print("y_test shape:" , y_test.shape)

    draw_line_separator()

    # Define models and their parameters
    # If we decide to add more ML classic models and compare them together, here we can add more classic ML models and their Hyper parameters in future
    models = {
    'Logistic Regression': (LogisticRegression()    , {'penalty'     : ['l2'], 'C': [0.01, 0.1, 1, 10], 'solver': ['lbfgs']}),
    'Decision Tree'      : (DecisionTreeClassifier(), {'max_depth'   : [2, 3, 5, 10, 20]              , 'min_samples_leaf': [5, 10, 20, 50, 100], 'criterion': ["gini", "entropy"]}),
    'SVM'                : (SVC(probability=True)   , {'C'           : [6, 7, 8, 9, 10, 11, 12]       , 'kernel': ['linear', 'rbf']}),
    'KNN'                : (KNeighborsClassifier()  , {'n_neighbors' : [5,6,7,8,9,10]                 , 'leaf_size': [1,2,3,5], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree','kd_tree','brute']}),
    'Random Forest'      : (RandomForestClassifier(), {'n_estimators': [100, 200, 300]                , 'max_depth': [None, 10, 20]}),
    'Linear Regression'  : (LinearRegression()      , {}) ,
    }

    # According to the goal of this project (identify the best model based on its performance, specifically its accuracy):
    # Define some variable to use at the end of this code to indicate best method name and it's accuracy and it's best parameters
    accu        = []
    methods     = []
    best_params = []

    # Train models and and evaluate each model and print their performances
    for model_name, (model, parameters) in models.items():

        print(f"⚡Training {model_name}...\n")

        """ IF model_name != 'Linear Regression' then use grid_search() and plot_confusion_matrix() and plot_roc_curve()
        to identify the accuracy of model"""
        if model_name != 'Linear Regression':

            # append the model_name
            methods.append(model_name)

            # Call grid_search() method via x_train, y_train
            cv = grid_search(model, parameters, x_train, y_train, CV_number_of_GridSearchCV)

            # print model_name and cv.best_params_
            print(f'⚡Best Possible Hyperparameters of {model_name} Model Based on x_train and y_train obtained via GridSearchCV Method are: \n', cv.best_params_)

            # identify the accuracy of model_name via grid_search().best_score_ method which it calcualted via x_train, y_train
            # To understand better the cv.best_score_ read the document at the top of this code carefully
            accuracy = cv.best_score_

            # append accuracy
            accu.append(accuracy)

            # append best_params (best possible hyper-parameters of this model_name)
            best_params.append(cv.best_params_)

            # print the accuracy of model
            print(f'⚡The Accuracy of {model_name} model which executed from from GridSearchCV() method and calculated by x_train and y_train : {accuracy}')

            # define confusion matrix via y_test and cv.predict(x_test)
            plot_confusion_matrix(y_test, cv.predict(x_test), model_name)

            # plot_roc_curve via y_test and cv.predict_proba(x_test)[:, 1]
            plot_roc_curve(y_test, cv.predict_proba(x_test)[:, 1], model_name)

            draw_line_separator()

        # if model_name == 'Linear Regression' then apply metrics.r2_score to find acciracy and plot linear regression model
        elif model_name == 'Linear Regression':

            # After evaluating other models, we add Linear Regression separately:
            linear_regression = LinearRegression()
            linear_regression.fit(x_train, y_train)
            y_pred_linear = linear_regression.predict(x_test)

            # for linear regression we have r2_score to identify the Accuracy of our linear regression model
            r2_score = metrics.r2_score(y_test, y_pred_linear)

            print(f'⚡The Accuracy of {model_name} model which calculated by x_train and y_train: {r2_score}')

            # append model_name and accuracy of our linear regression model to use at the end of the code and select best possible accuracy and it's name and it's hyper parameters
            methods.append(model_name)
            accu.append(r2_score)
            best_params.append({})

        # for situation that our model is not definded
        else:
            print('⚡The Model is not definded')

    draw_line_separator()

    # Display all methods and all accuracies in one figure in a horizontal style
    print('⚡⚡⚡This horizontal bar chart shows the best possible accuracies of different models as calculated by their best hyperparameters, \n which were calculated via x_train and y_train from the grid_search() method⚡⚡⚡'.upper() )

    draw_line_separator()

    # Define font styles for the plot
    title_font = { 'size': 16, 'weight': 'bold'}
    label_font = { 'size': 16}
    # Customize the figure and bar chart
    plt.figure(figsize=(10, 6))  # Adjust the figure size
    plt.barh(methods, accu, color='brown', height=0.4)  # Use barh for horizontal bars
    # Set font styles for axis labels
    plt.xlabel('Accuracy', **label_font)  # Swap xlabel and ylabel for horizontal bars
    plt.ylabel('Methods', **label_font)
    # Customize the title with font style
    plt.title('The best possible performances of all models are compared with each other in 1 figure \n to identify the best accuracy between all models on train data'.upper(), **title_font)
    # Show the plot
    plt.tight_layout()  # Ensure labels fit within the figure boundaries
    plt.show()

    draw_line_separator()

    # Get the best-performing method and its accuracy
    best_method   = methods[np.argmax(accu)]
    best_accuracy = accu[np.argmax(accu)]

    # PHASE 1: (FIND THE BEST POSSIBLE MODEL AND ITS BEST POSSIBLE HYPER-PARAMETERS)
    print(f'⚡⚡⚡PHASE 1: (FIND THE BEST POSSIBLE MODEL AND ITS BEST POSSIBLE HYPER-PARAMETERS):⚡⚡⚡')

    # When best method is NOT Linear Regression
    if best_method != 'Linear Regression':
      best_params_phase_1 = best_params[np.argmax(accu)]
      print(f'⚡The best-performing method, calculated using x_train and y_train from grid_search() method, is: {best_method}⚡'.upper())
      print(f'Best parameters of {best_method} method, calculated using x_train and y_train from grid_search() method, are: {best_params_phase_1}')
      print(f'Best accuracy of {best_method} method, calculated using x_train and y_train from grid_search() method, is: {best_accuracy}')
    # When best method is Linear Regression
    else:
      print(f'⚡The best-performing method, calculated using x_train and y_train from grid_search() method, is: {best_method}⚡'.upper())
      print(f'Best accuracy of {best_method} method, calculated using x_train and y_train from grid_search() method, is: {best_accuracy}')

    draw_line_separator()

    # NOW, LETS USE THE BEST MODEL AND ITS BEST HYPERPARAMETERS ON THE TEST SECTION OF THE DATASET TO EVALUATE THE ACCURACY OF ITS PERDICTION ON TEST DATA

    # Clear the model instance (reassign with a new instance)
    best_model_instance = None

    # Select the best model class based on best_method and initialize the model WITHOUT it's best_params_phase_1
    if best_method == 'Linear Regression':
        best_model_instance = LinearRegression()

    # Select the best model class based on best_method and initialize the model WITH it's best_params_phase_1
    elif best_method == 'Logistic Regression':
        best_model_instance = LogisticRegression(**best_params_phase_1)
    elif best_method == 'Decision Tree':
        best_model_instance = DecisionTreeClassifier(**best_params_phase_1)
    elif best_method == 'SVM':
        best_model_instance = SVC(probability=True, **best_params_phase_1)
    elif best_method == 'KNN':
        best_model_instance = KNeighborsClassifier(**best_params_phase_1)
    elif best_method == 'Random Forest':
        best_model_instance = RandomForestClassifier(**best_params_phase_1)

    # Fit the best model
    best_model_instance.fit(x_train, y_train)

    # Make predictions on the test set
    y_pred = best_model_instance.predict(x_test)

    # PHASE 2: (ESTIMATE THE HIGHEST POSSIBLE ACCURACY OF THE BEST SELECTED MODEL ON THE BASIS OF ITS BEST HYPER-PARAMETERS)
    print(f'⚡⚡⚡PHASE 2: (ESTIMATE THE HIGHEST POSSIBLE ACCURACY OF THE BEST SELECTED MODEL FROM PHASE 1 BASED ON ITS BEST HYPER-PARAMETERS WHICH CALCULATED VIA GridSearchCV() METHOD):⚡⚡⚡')

    # When best method is NOT Linear Regression
    if best_method != 'Linear Regression':

      print(f'{metrics.accuracy_score(y_test, y_pred)} is the best possible accuracy of {best_method} which came from test data section and its hyperparameters are {best_params_phase_1}.')

      # Plot the confusion matrix and ROC curve for the best model based on the test data set
      plot_confusion_matrix(y_test, y_pred, best_method)
      plot_roc_curve(y_test, best_model_instance.predict_proba(x_test)[:, 1], best_method)

      # Plot the Precision-Recall curve for the best model based on the test data set
      # plot_precision_recall_curve(y_test, best_model_instance.predict_proba(x_test)[:, 1], best_method)

      # For Decision Tree, visualize the tree
      if best_method == 'Decision Tree':
          plt.figure(figsize=(12, 6))
          plot_tree(best_model_instance, filled=True, feature_names=x_train.columns, class_names=True)
          plt.title(f'Decision Tree')
          plt.show()

      # For Random Forest, plot feature importances
      if best_method == 'Random Forest':
          feature_importances = best_model_instance.feature_importances_
          plt.figure(figsize=(10, 6))
          sns.barplot(x=feature_importances, y=x_train.columns)
          plt.title(f'Feature Importances for {best_method}')
          plt.xlabel('Feature Importance')
          plt.ylabel('Features')
          plt.show()

    # When best method is Linear Regression
    else:
      print(f'{metrics.r2_score(y_test, y_pred)} is the best possible accuracy of {best_method} which came from test data section.')

    draw_line_separator()

    # PHASE 3: (FIND THE HIGHEST POSSIBLE ACCURACY FROM ALL AVAILABLE MODELS BASED ON THEIR BEST HYPER-PARAMETERS ON THE MAIN TEST DATA)
    print('⚡⚡⚡PHASE 3: (FIND THE HIGHEST POSSIBLE ACCURACY FROM ALL AVAILABLE MODELS BASED ON THEIR BEST HYPER-PARAMETERS ON THE MAIN TEST DATA)⚡⚡⚡')

    print(f'ALL INFORMATION BASED ON PREVIOUS CALCULATION WE HAVE, ARE:\n{methods}\n{accu}\n{best_params}\n')

    """
    Make a for loop on all methods and all accu and all best_params and
    Then train all models via x_train, y_train and
    Then fit all the models based on their best_params and
    Then calculate the accuracy of all models based on y_test and x_test and
    Then find the best possible accuracy and
    Then make ROC Curves and Confusion Matrix and plot to demonstrate the best possible accuracy
    """

    # Create dictionaries to store the results for each model
    model_results = {}

    # Make a for loop on methods and accu and best_params
    for model_name, best_param in zip(methods, best_params):

        print(f"⚡Training and evaluating {model_name}...\n")

        # Initialize the model with its best parameters
        if model_name == 'Linear Regression' or model_name == 'LinearRegression':
            model_instance = LinearRegression()
        elif model_name == 'Logistic Regression':
            model_instance = LogisticRegression(**best_param)
        elif model_name == 'Decision Tree':
            model_instance = DecisionTreeClassifier(**best_param)
        elif model_name == 'SVM':
            model_instance = SVC(probability=True, **best_param)
        elif model_name == 'KNN':
            model_instance = KNeighborsClassifier(**best_param)
        elif model_name == 'Random Forest':
            model_instance = RandomForestClassifier(**best_param)

        # Train the model on the full training dataset
        model_instance.fit(x_train, y_train)

        # Predict on the test dataset
        y_pred = model_instance.predict(x_test)

        # Calculate the accuracy on the test dataset
        if model_name != 'Linear Regression' and model_name != 'LinearRegression':
            accuracy = metrics.accuracy_score(y_test, y_pred)
        else:
            accuracy = metrics.r2_score(y_test, y_pred)

        # Store the accuracy in the model_results dictionary
        model_results[model_name] = accuracy

        print(f"⚡The accuracy of {model_name} on the test data is: {accuracy}\n".upper())

        draw_line_separator()

    # Find the best possible accuracy and its corresponding model
    best_model_name = max(model_results, key=model_results.get)
    best_accuracy   = model_results[best_model_name]

    # Make a plot to demonstrate the best possible accuracy
    plt.figure(figsize=(10, 6))
    plt.barh(list(model_results.keys()), list(model_results.values()), color='blue', height=0.4)
    plt.xlabel('Accuracy')
    plt.ylabel('Model')
    plt.title(f'⚡Best Accuracy: {best_accuracy:.4f} achieved by {best_model_name}'.upper())
    plt.show()

    draw_line_separator()

    # Print the best model and its accuracy
    print(f'⚡The best-performing model on the test data is: {best_model_name}'.upper())
    print(f'⚡Best accuracy achieved on the test data is: {best_accuracy}'.upper())

    draw_line_separator()

    # PHASE 4.1: (CLUSTERING SECTION K-means++)
    clustering_k_means_plus_plus_section(clustering_feature_names,data)

    draw_line_separator()

    # PHASE 4.2: (CLUSTERING SECTION Hierarchical)
    clustering_hierarchical_section(data)

    draw_line_separator()

    # PHASE 4.3: (CLUSTERING SECTION DBSCAN)
    clustering_DBSCAN_section(clustering_feature_names,data,CV_number_of_GridSearchCV,min_sample_of_clustering_dbscan,eps_of_clustering_dbscan)

    draw_line_separator()

# Run the main function
if __name__ == "__main__":
    main()
